# Data Pipeline with Kafka, Logstash, Elasticsearch, and Kibana

این پروژه یک خط لوله داده را پیاده‌سازی می‌کند که داده‌های JSON خام را از طریق Kafka و Logstash پردازش کرده و برای نمایش در Kibana در Elasticsearch ذخیره می‌کند.

## ساختار پروژه

```
.
├── config
│   └── logstash.conf
├── docker-compose.yml
├── setup_kibana.sh
├── scripts
│   ├── data
│   │   └── data.json
│   └── producer.py
└── README.md
```

## پیش‌نیازها

* Docker و Docker Compose نصب شده روی سیستم شما
* Python 3.x (برای اجرای اسکریپت تولیدکننده)

## نحوه اجرای پروژه

1. **راه‌اندازی سرویس‌ها**:
```bash
docker compose up -d
```

2. **صبر کنید تا همه سرویس‌ها راه‌اندازی شوند** (ممکن است چند دقیقه طول بکشد). می‌توانید وضعیت را با این دستور بررسی کنید:
```bash
docker compose logs -f
```

3. **اسکریپت تولیدکننده Python را اجرا کنید** تا داده‌ها را به Kafka ارسال کنید:
```bash
python scripts/producer.py
```

4. **دسترسی به Kibana** برای مشاهده داده‌های پردازش شده:
    * مرورگر خود را باز کنید و به آدرس: `http://localhost:5601` بروید
    * در منوی سمت چپ به "Discover" بروید
    * الگوی ایندکس "test_pipeline" را انتخاب کنید

## جزئیات پردازش داده

خط لوله تبدیل‌های زیر را انجام می‌دهد:

1. **تبدیل انواع داده**:
    * تبدیل `id` از رشته به عدد صحیح
    * تبدیل `active` از رشته به بولین
    * مقادیر خالی `full_name` به `null` تنظیم می‌شوند

2. **تغییر نام و حذف فیلدها**:
    * `full_name` به `name` تغییر نام داده می‌شود
    * `extra_field` کاملاً حذف می‌شود

3. **استانداردسازی مهر زمانی**:
    * تبدیل فرمت‌های مختلف زمانی به فرمت ISO8601
    * ذخیره در Elasticsearch به عنوان فیلد `@timestamp`

## خروجی مورد انتظار

داده‌های پردازش شده در Elasticsearch به این شکل خواهد بود:

```json
[
  {"id": 123, "name": "Ali", "@timestamp": "2025-01-31T12:34:56Z", "active": true},
  {"id": 124, "name": null, "@timestamp": "2025-01-31T14:20:00Z", "active": false},
  {"id": 125, "name": "Sara", "@timestamp": "2025-02-01T09:15:30Z", "active": true}
]
```

## مراحل تأیید

1. بررسی کنید که هر 3 رکورد در Kibana نمایش داده شوند
2. تأیید کنید که:
    * همه شناسه‌ها عدد هستند (نه رشته)
    * نام خالی به صورت null نمایش داده می‌شود
    * همه مهرهای زمانی در فرمت ISO هستند
    * "extra_field" موجود نیست
    * وضعیت فعال به صورت true/false نمایش داده می‌شود (نه رشته)

## عیب‌یابی

اگر با مشکلاتی مواجه شدید:

1. لاگ‌های کانتینر را بررسی کنید: `docker-compose logs -f [service_name]`
2. تأیید کنید که همه کانتینرها در حال اجرا هستند: `docker-compose ps`
3. مطمئن شوید که اسکریپت تولیدکننده می‌تواند به Kafka متصل شود (پورت 29092 را بررسی کنید)
4. ایندکس‌های Elasticsearch را بررسی کنید: `curl http://localhost:9200/_cat/indices?v`

## پاکسازی

برای توقف و حذف همه کانتینرها:

```bash
docker-compose down
```

## نکات پیاده‌سازی

* خط لوله، فرمت‌های مختلف تاریخ را مدیریت کرده و آنها را به فرمت استاندارد ISO تبدیل می‌کند
* رشته‌های خالی به درستی به مقادیر null تبدیل می‌شوند
* راه‌اندازی شامل ایجاد خودکار الگوی ایندکس Kibana است
* بررسی‌های سلامت، شروع سرویس‌ها را به ترتیب صحیح تضمین می‌کنند

این پروژه با موفقیت تمام الزامات مشخص شده در وظیفه را برآورده می‌کند و یک خط لوله داده کامل از JSON خام تا داده‌های نمایش داده شده در Kibana ارائه می‌دهد.