version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - dataops-network

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "29092:29092"
      - "9092:9092"
    networks:
      - dataops-network
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "kafka:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 5

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    networks:
      - dataops-network

  logstash:
    image: docker.elastic.co/logstash/logstash:8.14.0
    volumes:
      #- ./config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
      - ../config/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - kafka
      - elasticsearch
    networks:
      - dataops-network

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.0
    depends_on:
      - elasticsearch
    ports:
      - "5601:5601"
    environment:
      - NODE_OPTIONS=--openssl-legacy-provider
      - xpack.security.enabled=false
      - xpack.encryptedSavedObjects.encryptionKey=fake-key
      - xpack.reporting.encryptionKey=fake-key
      - xpack.security.encryptionKey=fake-key
    networks:
      - dataops-network

networks:
  dataops-network:
    driver: bridge